{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports and definition of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import datetime \n",
    "\n",
    "input_path = \"train\"\n",
    "output_path =\"./preprocessed/\"\n",
    "\n",
    "prediction_interval=5\n",
    "look_back_interval=10\n",
    "all_weather = False\n",
    "\n",
    "output = pathlib.Path(output_path)\n",
    "output.mkdir(parents=True, exist_ok=True)\n",
    "input = pathlib.Path(input_path)\n",
    "\n",
    "kpi_file= input/\"rl-kpis.tsv\"\n",
    "distances_file = input/\"distances.tsv\"\n",
    "met_forecast_file = input/\"met-forecast.tsv\"\n",
    "rl_sites_file = input/\"rl-sites.tsv\"\n",
    "met_stations_file = input/\"met-stations.tsv\"\n",
    "met_real_file = input/\"met-real.tsv\"\n",
    "\n",
    "ordinal_enconder = OrdinalEncoder(categories=[['clear sky', 'hot day', 'scattered clouds', 'few clouds', 'overcast clouds','foggy', 'windy', 'misty', 'light rain', \n",
    "                                              'light rain showers', 'light intensity shower rain', 'light snow','snow', 'sleet',\n",
    "                                                'rain', 'heavy rain showers','heavy rain', \n",
    "                                              'thunderstorm with heavy rain', 'heavy thunderstorm with rain showers']],\n",
    "                                  \n",
    "                                  handle_unknown='use_encoded_value',\n",
    "                                  unknown_value=np.nan\n",
    "                                  )\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find K closest stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_closest_stations(rl_sites_file, distances_file, met_stations_file, k=3):\n",
    "    rl_sites_df = pd.read_csv(rl_sites_file, sep=\"\\t\", index_col=0)\n",
    "    distances_df = pd.read_csv(distances_file, sep=\"\\t\", index_col=0)\n",
    "    met_stations_df = pd.read_csv(met_stations_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "    rl_stations = rl_sites_df[\"site_id\"].unique()\n",
    "    met_stations = met_stations_df[\"station_no\"].unique()\n",
    "\n",
    "    distances_df = distances_df.loc[met_stations, rl_stations]\n",
    "\n",
    "    closest_stations = dict()\n",
    "    for rl_station in rl_stations:\n",
    "        closest_stations[rl_station] = set(distances_df.nsmallest(k, [rl_station]).index.tolist())\n",
    "\n",
    "    return closest_stations\n",
    "\n",
    "#select most frequent option\n",
    "def select_frequent(series):\n",
    "    frequent = pd.Series.mode(series)\n",
    "    if len(frequent) > 1:\n",
    "        return frequent[1]\n",
    "    elif len(frequent) > 0:\n",
    "        return frequent[0]\n",
    "    \n",
    "    return np.NaN\n",
    "\n",
    "def timeseries_processing(time_sentitive_dataset, time_sentitive_features, identifiers, labels, output, df_type = \"train\"):\n",
    "    ordered_features = []\n",
    "    for i in range(1-look_back_interval, 0, 1):\n",
    "        for feature in time_sentitive_features:\n",
    "            ordered_features.append(f\"T{i}_{feature}\")\n",
    "\n",
    "    ordered_features += time_sentitive_features\n",
    "\n",
    "    for feature in time_sentitive_features:\n",
    "        historical_sen_dataset = time_sentitive_dataset.loc[:, identifiers]\n",
    "        for i in range(-1,  -look_back_interval, -1):\n",
    "            historical_sen_dataset[f\"T{i}\"] = historical_sen_dataset[\"datetime\"] + pd.DateOffset(days=i)\n",
    "\n",
    "        feature_view = time_sentitive_dataset[identifiers + [feature]]\n",
    "        for i in range(-1,  -look_back_interval, -1):\n",
    "            target_day_column_name = f\"T{i}\"\n",
    "\n",
    "            historical_sen_dataset = historical_sen_dataset.merge(feature_view, \n",
    "                    how = \"left\", \n",
    "                    left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                    right_on = identifiers,\n",
    "                    suffixes = (\"\", \"_y\")\n",
    "            )\n",
    "            historical_sen_dataset.rename(columns={ feature: f\"{target_day_column_name}_{feature}\"}, inplace=True)\n",
    "\n",
    "        historical_sen_dataset.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "        historical_sen_dataset.drop(columns=[f\"T{i}\" for i in range(-1,  -look_back_interval, -1)], inplace=True)\n",
    "\n",
    "        time_sentitive_dataset = time_sentitive_dataset.merge(historical_sen_dataset, \n",
    "                    how=\"left\", \n",
    "                    on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "    time_sentitive_dataset = time_sentitive_dataset.dropna()\n",
    "    time_sentitive_dataset[labels] = time_sentitive_dataset[labels].astype(int)\n",
    "\n",
    "    time_sentitive_dataset = time_sentitive_dataset[identifiers + ordered_features + labels]\n",
    "\n",
    "    if df_type != \"train\":\n",
    "        if len(time_sentitive_dataset) > 1:\n",
    "            np.savetxt(output/f\"x_{df_type}.csv\", time_sentitive_dataset[ordered_features].values, delimiter=\",\", fmt=\"%5.2f\")\n",
    "            np.savetxt(output/f\"y_{df_type}.csv\", time_sentitive_dataset[[\"5-day-predict\"]].values, delimiter=\",\", fmt=\"%d\")\n",
    "    else:\n",
    "        output = output/\"train\"\n",
    "        output.mkdir(parents=True, exist_ok=True)\n",
    "        rl_mlid_combos =  (time_sentitive_dataset[\"site_id\"] + \"%\" + time_sentitive_dataset[\"mlid\"]).unique()\n",
    "\n",
    "        for rl_mlid in rl_mlid_combos:\n",
    "            site_id, mlid = rl_mlid.split(\"%\")\n",
    "\n",
    "            rl_mlid_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"site_id\"] == site_id) & (time_sentitive_dataset[\"mlid\"] == mlid)]\n",
    "            \n",
    "            if len(rl_mlid_df) > 1:\n",
    "                site_folder = output / site_id\n",
    "                site_folder.mkdir(parents=True, exist_ok=True)\n",
    "                rl_mlid_df.to_csv(site_folder/f\"{mlid}_time_sentitive_features.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading forecasting data\n",
    "met_forecast_df = pd.read_csv(met_forecast_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "met_forecast_df[\"datetime\"] = pd.to_datetime(met_forecast_df[\"datetime\"])\n",
    "\n",
    "# Filtering the reports to only include the morning report, and removing that column afterwards. (usually there are morning and afternoon reports)\n",
    "met_forecast_df = met_forecast_df.groupby(by=[\"station_no\", \"datetime\"], group_keys=False).agg(select_frequent)\n",
    "met_forecast_df.reset_index(level=[\"station_no\", \"datetime\"], inplace=True)\n",
    "met_forecast_df.drop(columns=[\"report_time\"], inplace=True)\n",
    "\n",
    "#Filtering between 5 days of forecast or just day 5\n",
    "if not all_weather:\n",
    "    columns = [column for column in met_forecast_df.columns if \"day5\" not in column and column not in [\"station_no\", \"datetime\"]]\n",
    "    met_forecast_df.drop(columns=columns, inplace=True)\n",
    "\n",
    "## Transforming weather features in ordinal encoding vectors\n",
    "if all_weather:\n",
    "    weather_features = [f\"weather_day{i}\" for i in range(1,6)]\n",
    "else:\n",
    "    weather_features = [\"weather_day5\"]\n",
    "\n",
    "for weather_feature in weather_features:\n",
    "    met_forecast_df[weather_feature] = ordinal_enconder.fit_transform(met_forecast_df[weather_feature].to_numpy().reshape(-1,1).tolist()) \n",
    "\n",
    "# Loading the real data and removing the hour of when it was collected\n",
    "met_real_df = pd.read_csv(met_real_file, sep=\"\\t\", index_col=0)\n",
    "met_real_df.drop(columns=[\"datetime\", \"measured_hour\"], inplace=True)\n",
    "\n",
    "met_real_df[\"measured_date\"] = pd.to_datetime(met_real_df[\"measured_date\"])\n",
    "met_real_df.rename(columns={\"measured_date\":\"datetime\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading KPI file & finding the closest stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the closest met station to each radio link one.\n",
    "closest_stations = finding_closest_stations(rl_sites_file, distances_file, met_stations_file)\n",
    "\n",
    "#List of columns that uniquelly identify an entry in the kpi_df\n",
    "identifiers = [\"site_id\", \"mlid\", \"datetime\"]\n",
    "\n",
    "kpi_df = pd.read_csv(kpi_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "#Transform the datetime column to the correct format\n",
    "kpi_df[\"datetime\"] = pd.to_datetime(kpi_df[\"datetime\"])\n",
    "\n",
    "#Remove unnecessary columns\n",
    "kpi_df.drop(columns=[\"mw_connection_no\"], inplace=True)\n",
    "\n",
    "if \"scalibility_score\" in kpi_df.columns:\n",
    "    kpi_df.drop(columns=[\"scalibility_score\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating real and forecast weather for each site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_real_agg_df = None\n",
    "#Adding closest station to each entry according to the id of the radio link\n",
    "for site_id in kpi_df[\"site_id\"].unique():\n",
    "    temp_df = met_real_df[met_real_df[\"station_no\"].isin(closest_stations[site_id])]\\\n",
    "                    .drop(columns=[\"station_no\"])   \\\n",
    "                    .groupby(by=[\"datetime\"], group_keys=False).agg(['min', 'max', 'mean', 'std'])\n",
    "    \n",
    "    temp_df.reset_index(level=[\"datetime\"], inplace=True)\n",
    "    temp_df[\"site_id\"] = site_id\n",
    "\n",
    "    if type(met_real_agg_df) == pd.DataFrame:\n",
    "        met_real_agg_df = pd.concat([met_real_agg_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        met_real_agg_df = temp_df\n",
    "\n",
    "#Transforming multi-level columns to single level\n",
    "met_real_agg_df.columns = [\"_\".join(x)  if x[1] != '' else x[0] for x in met_real_agg_df.columns]\n",
    "\n",
    "#Mergin k closest stations to each entry according to the id of the radio link\n",
    "\n",
    "met_forecast_agg_df = None\n",
    "\n",
    "for site_id in kpi_df[\"site_id\"].unique():\n",
    "    filtered_df = met_forecast_df[met_forecast_df[\"station_no\"].isin(closest_stations[site_id])]\\\n",
    "                    .drop(columns=[\"station_no\"])\n",
    "                    \n",
    "    temp_df = filtered_df.drop(columns=[\"weather_day5\"]).groupby(by=[\"datetime\"], group_keys=False).mean()\n",
    "\n",
    "    temp_df[\"weather_day5\"] = filtered_df[[\"datetime\", \"weather_day5\"]].groupby(by=[\"datetime\"], group_keys=False) \\\n",
    "                    .agg(select_frequent)[\"weather_day5\"]    \n",
    "    \n",
    "    temp_df.reset_index(level=[\"datetime\"], inplace=True)\n",
    "    temp_df[\"site_id\"] = site_id\n",
    "\n",
    "    if type(met_forecast_agg_df) == pd.DataFrame:\n",
    "        met_forecast_agg_df = pd.concat([met_forecast_agg_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        met_forecast_agg_df = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging KPI df with met forecast and real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging kpis with forecast data\n",
    "kpi_df = kpi_df.merge(met_forecast_agg_df, on=[\"datetime\", \"site_id\"])\n",
    "kpi_df = kpi_df.merge(met_real_agg_df, on=[\"datetime\", \"site_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the labels for each entry (1-day after and 5-days after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = kpi_df.loc[:, identifiers]\n",
    "\n",
    "for i in range(prediction_interval):\n",
    "    df_labels[f\"T+{i+1}\"] = df_labels[\"datetime\"] + pd.DateOffset(days=i+1)\n",
    "\n",
    "df_labels_view = kpi_df[identifiers + [\"rlf\"]]\n",
    "for i in range(prediction_interval):\n",
    "    target_day_column_name = f\"T+{i+1}\"\n",
    "\n",
    "    df_labels = df_labels.merge(df_labels_view, \n",
    "                how = \"left\", \n",
    "                left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                right_on = identifiers,\n",
    "                suffixes = (\"\", \"_y\")\n",
    "    )\n",
    "    df_labels.rename(columns={\"rlf\": f\"{target_day_column_name}_rlf\"}, inplace=True)\n",
    "\n",
    "df_labels.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "df_labels[\"1-day-predict\"] = df_labels[\"T+1_rlf\"]\n",
    "\n",
    "df_labels[\"5-day-predict\"] = df_labels[[f\"T+{i+1}_rlf\" for i in range(prediction_interval)]].any(axis=1)\n",
    "\n",
    "df_labels = df_labels[[\"datetime\", \"site_id\", \"mlid\", \"1-day-predict\", \"5-day-predict\"]]\n",
    "\n",
    "kpi_df = kpi_df.merge(df_labels, \n",
    "                                how=\"left\", \n",
    "                                on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "\n",
    "static_features = [\"card_type\", \"freq_band\", \"type\", \"tip\", \"adaptive_modulation\", \"freq_band\", \"modulation\"]\n",
    "labels = [\"rlf\", \"1-day-predict\", \"5-day-predict\"]\n",
    "time_sentitive_features = [feature for feature in kpi_df.columns if feature not in static_features and feature not in labels and feature not in identifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating static / time dependent features and creating the static dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sentitive_dataset = kpi_df.loc[:, identifiers + time_sentitive_features + labels]\n",
    "\n",
    "static_dataset = kpi_df.loc[:, identifiers + static_features + labels]\n",
    "static_dataset = static_dataset.dropna()\n",
    "\n",
    "one_hot_encoder.fit(static_dataset[static_features])\n",
    "\n",
    "static_dataset = pd.concat([static_dataset,\n",
    "                            pd.DataFrame(one_hot_encoder.transform(static_dataset[static_features]).toarray(),\n",
    "                                                         columns=one_hot_encoder.get_feature_names_out())\n",
    "                            ],\n",
    "                        axis=1\n",
    "                        )\n",
    "\n",
    "static_dataset.drop(columns=static_features, inplace=True)\n",
    "\n",
    "static_dataset.to_csv(output/\"preprocessed_static_features.csv\", index=None)\n",
    "time_sentitive_dataset.to_csv(output/\"preprocessed_timeseries.csv\", index=None)\n",
    "\n",
    "mean_values = time_sentitive_dataset[time_sentitive_features].mean()\n",
    "std_values = time_sentitive_dataset[time_sentitive_features].std()\n",
    "\n",
    "time_sentitive_dataset[time_sentitive_features] = (time_sentitive_dataset[time_sentitive_features] - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing between train/cv/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = time_sentitive_dataset.loc[time_sentitive_dataset[\"datetime\"] < datetime.datetime(2020, 6,1)]\n",
    "cv_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"datetime\"] >= datetime.datetime(2020, 6,1)) & (time_sentitive_dataset[\"datetime\"] < datetime.datetime(2020, 10,1) )]\n",
    "test_df = time_sentitive_dataset.loc[time_sentitive_dataset[\"datetime\"] >= datetime.datetime(2020, 10,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1 = train_df[time_sentitive_features].quantile(0.25)\n",
    "#q3 = train_df[time_sentitive_features].quantile(0.75)\n",
    "#iqr =  q3 - q1\n",
    "#max_value = q3 + iqr*1.5\n",
    "#min_value = q1 - iqr*1.5\n",
    "#\n",
    "#train_df = train_df[((train_df[time_sentitive_features] > min_value) & (train_df[time_sentitive_features] < max_value)).all(axis=1)]\n",
    "#cv_df = cv_df[((cv_df[time_sentitive_features] > min_value) & (cv_df[time_sentitive_features] < max_value)).all(axis=1)]\n",
    "#test_df = test_df[((test_df[time_sentitive_features] > min_value) & (test_df[time_sentitive_features] < max_value)).all(axis=1)]\n",
    "\n",
    "#mean_values = train_df[time_sentitive_features].mean()\n",
    "#std_values = train_df[time_sentitive_features].std()\n",
    "#\n",
    "#train_df.loc[:, time_sentitive_features] = (train_df[time_sentitive_features] - mean_values) / std_values\n",
    "#\n",
    "#cv_df.loc[:, time_sentitive_features] = (cv_df[time_sentitive_features] - mean_values) / std_values\n",
    "#\n",
    "#test_df.loc[:, time_sentitive_features] = (test_df[time_sentitive_features] - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating timeseries vectors for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_processing(cv_df, time_sentitive_features, identifiers, labels, output, df_type = \"cv\")\n",
    "timeseries_processing(test_df, time_sentitive_features, identifiers, labels, output, df_type = \"test\")\n",
    "timeseries_processing(train_df, time_sentitive_features, identifiers, labels, output, df_type = \"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow_vs_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
