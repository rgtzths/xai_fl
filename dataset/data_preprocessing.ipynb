{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_closest_stations(rl_sites_file, distances_file, met_stations_file, k=1):\n",
    "    rl_sites_df = pd.read_csv(rl_sites_file, sep=\"\\t\", index_col=0)\n",
    "    distances_df = pd.read_csv(distances_file, sep=\"\\t\", index_col=0)\n",
    "    met_stations_df = pd.read_csv(met_stations_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "    rl_stations = rl_sites_df[\"site_id\"].unique()\n",
    "    met_stations = met_stations_df[\"station_no\"].unique()\n",
    "\n",
    "    distances_df = distances_df.loc[met_stations, rl_stations]\n",
    "\n",
    "    closest_stations = dict(distances_df.idxmin())\n",
    "\n",
    "    return closest_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files and merging met data with the KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_file, distances_file, met_forecast_file, rl_sites_file, met_stations_file, output_path =\"train/rl-kpis.tsv\", \"train/distances.tsv\", \"train/met-forecast.tsv\", \"train/rl-sites.tsv\",  \"train/met-stations.tsv\", \"./preprocessed_dataset\"\n",
    "prediction_interval=5\n",
    "look_back_interval=10\n",
    "\n",
    "output = pathlib.Path(output_path)\n",
    "output.mkdir(parents=True, exist_ok=True)\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='error')\n",
    "\n",
    "#Get the closest met station to each radio link one.\n",
    "closest_stations = finding_closest_stations(rl_sites_file, distances_file, met_stations_file)\n",
    "\n",
    "#List of columns that uniquelly identify an entry in the kpi_df\n",
    "identifiers = [\"site_id\", \"mlid\", \"datetime\"]\n",
    "\n",
    "kpi_df = pd.read_csv(kpi_file, sep=\"\\t\", index_col=0)\n",
    "met_forecast_df = pd.read_csv(met_forecast_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "#Transform the datetime column to the correct format\n",
    "kpi_df[\"datetime\"] = pd.to_datetime(kpi_df[\"datetime\"])\n",
    "met_forecast_df[\"datetime\"] = pd.to_datetime(met_forecast_df[\"datetime\"])\n",
    "\n",
    "# Filtering the reports to only include the morning report, and removing that column afterwards. (usually there are morning and afternoon reports)\n",
    "met_forecast_df = met_forecast_df[ met_forecast_df[\"report_time\"] == \"morning\"]\n",
    "met_forecast_df.drop(columns=[\"report_time\"], inplace=True)\n",
    "\n",
    "#Adding closest station to each entry according to the id of the radio link\n",
    "kpi_df[\"station_no\"] = [closest_stations[site_id] for  site_id in kpi_df[\"site_id\"]]\n",
    "\n",
    "# Merging kpis with forecast data\n",
    "kpi_df = kpi_df.merge(met_forecast_df, how=\"left\", on=[\"datetime\", \"station_no\"])\n",
    "\n",
    "#Remove unnecessary columns\n",
    "kpi_df.drop(columns=[\"station_no\", \"mw_connection_no\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming weather features in one hot encoding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features = [f\"weather_day{i}\" for i in range(1,6)]\n",
    "\n",
    "weather_classes = set()\n",
    "\n",
    "for weather_feature in weather_features:\n",
    "    weather_classes.update(set(kpi_df[weather_feature].unique()))\n",
    "\n",
    "classes = np.array(list(weather_classes))\n",
    "\n",
    "classes = classes.reshape(-1, 1)\n",
    "one_hot_encoder.fit(classes)\n",
    "for weather_feature in weather_features:\n",
    "    columns = one_hot_encoder.get_feature_names_out([weather_feature])\n",
    "    kpi_df = pd.concat([kpi_df,\n",
    "                pd.DataFrame(one_hot_encoder.transform(kpi_df[weather_feature].to_numpy(dtype=str).reshape(-1,1)).toarray(),\n",
    "                            columns=columns)\n",
    "                ],\n",
    "                axis=1)\n",
    "\n",
    "kpi_df.drop(columns=weather_features, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the labels for each entry (1-day after and 5-days after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_labels = kpi_df.loc[:, identifiers]\n",
    "\n",
    "for i in range(prediction_interval):\n",
    "    df_labels[f\"T+{i+1}\"] = df_labels[\"datetime\"] + pd.DateOffset(days=i+1)\n",
    "\n",
    "df_labels_view = kpi_df[identifiers + [\"rlf\"]]\n",
    "for i in range(prediction_interval):\n",
    "    target_day_column_name = f\"T+{i+1}\"\n",
    "\n",
    "    df_labels = df_labels.merge(df_labels_view, \n",
    "                how = \"left\", \n",
    "                left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                right_on = identifiers,\n",
    "                suffixes = (\"\", \"_y\")\n",
    "    )\n",
    "    df_labels.rename(columns={\"rlf\": f\"{target_day_column_name}_rlf\"}, inplace=True)\n",
    "\n",
    "df_labels.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "df_labels[\"1-day-predict\"] = df_labels[\"T+1_rlf\"]\n",
    "\n",
    "df_labels[\"5-day-predict\"] = df_labels[[f\"T+{i+1}_rlf\" for i in range(prediction_interval)]].any(axis=1)\n",
    "\n",
    "df_labels = df_labels[[\"datetime\", \"site_id\", \"mlid\", \"1-day-predict\", \"5-day-predict\"]]\n",
    "\n",
    "kpi_df = kpi_df.merge(df_labels, \n",
    "                                how=\"left\", \n",
    "                                on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "\n",
    "static_features = [\"card_type\", \"freq_band\", \"type\", \"tip\", \"adaptive_modulation\", \"freq_band\", \"modulation\"]\n",
    "labels = [\"rlf\", \"1-day-predict\", \"5-day-predict\"]\n",
    "time_sentitive_features = [feature for feature in kpi_df.columns if feature not in static_features and feature not in labels and feature not in identifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating static / time dependent features and creating the static dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sentitive_dataset = kpi_df.loc[:, identifiers + time_sentitive_features + labels]\n",
    "\n",
    "static_dataset = kpi_df.loc[:, identifiers + static_features + labels]\n",
    "static_dataset = static_dataset.dropna()\n",
    "\n",
    "one_hot_encoder.fit(static_dataset[static_features])\n",
    "\n",
    "static_dataset = pd.concat([static_dataset,\n",
    "                            pd.DataFrame(one_hot_encoder.transform(static_dataset[static_features]).toarray(),\n",
    "                                                         columns=one_hot_encoder.get_feature_names_out())\n",
    "                            ],\n",
    "                        axis=1\n",
    "                        )\n",
    "\n",
    "static_dataset.drop(columns=static_features, inplace=True)\n",
    "\n",
    "static_dataset.to_csv(output/\"preprocessed_static_features.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating auxiliary arrays for timeseries preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [feature for feature in time_sentitive_dataset.columns if feature not in labels and feature not in identifiers and \"T-\" not in feature and \"nan\" not in feature]\n",
    "\n",
    "ordered_features = base_features.copy()\n",
    "for i in range(-1, -look_back_interval, -1):\n",
    "    for feature in base_features:\n",
    "        ordered_features.append(f\"T{i}_{feature}\")\n",
    "\n",
    "\n",
    "rl_mlid_combos =  (time_sentitive_dataset[\"site_id\"] + \"-\" + time_sentitive_dataset[\"mlid\"]).unique()\n",
    "\n",
    "\n",
    "weather_nan_features = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(0,  -look_back_interval, -1):\n",
    "        if j == 0:\n",
    "            weather_features.append(f\"weather_day{i}_nan\")\n",
    "        else:\n",
    "            weather_features.append(f\"T{j}_weather_day{i}_nan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the timeseries vector (10days prior) by rl and mlid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL_[MBB--A7PT\n"
     ]
    }
   ],
   "source": [
    "for rl_mlid in rl_mlid_combos:\n",
    "    try:\n",
    "        site_id, mlid = rl_mlid.split(\"-\")\n",
    "\n",
    "        rl_mlid_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"site_id\"] == site_id) & (time_sentitive_dataset[\"mlid\"] == mlid)]\n",
    "\n",
    "        for feature in time_sentitive_features:\n",
    "            historical_sen_dataset = rl_mlid_df.loc[:, identifiers]\n",
    "            for i in range(-1,  -look_back_interval, -1):\n",
    "                historical_sen_dataset[f\"T{i}\"] = historical_sen_dataset[\"datetime\"] + pd.DateOffset(days=i)\n",
    "\n",
    "            feature_view = rl_mlid_df[identifiers + [feature]]\n",
    "            for i in range(-1,  -look_back_interval, -1):\n",
    "                target_day_column_name = f\"T{i}\"\n",
    "\n",
    "                historical_sen_dataset = historical_sen_dataset.merge(feature_view, \n",
    "                        how = \"left\", \n",
    "                        left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                        right_on = identifiers,\n",
    "                        suffixes = (\"\", \"_y\")\n",
    "                )\n",
    "                historical_sen_dataset.rename(columns={ feature: f\"{target_day_column_name}_{feature}\"}, inplace=True)\n",
    "\n",
    "            historical_sen_dataset.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "            historical_sen_dataset.drop(columns=[f\"T{i}\" for i in range(-1,  -look_back_interval, -1)], inplace=True)\n",
    "\n",
    "            rl_mlid_df = rl_mlid_df.merge(historical_sen_dataset, \n",
    "                        how=\"left\", \n",
    "                        on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "\n",
    "        #Remove nan weather columns and positive entries\n",
    "        for weather_feature in weather_nan_features:\n",
    "            rl_mlid_df = rl_mlid_df.loc[rl_mlid_df[weather_feature] != 1]\n",
    "            rl_mlid_df.drop(columns=[weather_feature], inplace=True)\n",
    "\n",
    "        rl_mlid_df = rl_mlid_df.dropna()\n",
    "        rl_mlid_df[labels] = rl_mlid_df[labels].astype(int)\n",
    "\n",
    "        rl_mlid_df = rl_mlid_df[identifiers + ordered_features + labels]\n",
    "\n",
    "        site_folder = output / site_id\n",
    "        site_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        rl_mlid_df.to_csv(site_folder/f\"{mlid}_time_sentitive_features.csv\", index=None)\n",
    "    except:\n",
    "        print(rl_mlid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow_vs_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
