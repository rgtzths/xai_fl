{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "rus = RandomUnderSampler(random_state=seed, sampling_strategy=0.05)\n",
    "\n",
    "dataset_path = \"./preprocessed/train\"\n",
    "\n",
    "dataset =  pathlib.Path(dataset_path)\n",
    "\n",
    "clusters = [x for x in dataset.iterdir() if x.is_dir()]\n",
    "\n",
    "identifiers = [\"site_id\", \"mlid\", \"datetime\"]\n",
    "\n",
    "labels = [\"rlf\", \"1-day-predict\", \"5-day-predict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate for N workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining subset's files\n",
      "Undersampling\n"
     ]
    }
   ],
   "source": [
    "n_divisions = 4\n",
    "output_path = f\"./divided/train/{n_divisions}_workers\"\n",
    "\n",
    "output = pathlib.Path(output_path)\n",
    "output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.shuffle(clusters)\n",
    "\n",
    "subset_size = len(clusters)//n_divisions\n",
    "\n",
    "cluster_sets = [ [clusters[i], clusters[i+1]] for i in range(0, subset_size*n_divisions, subset_size)]\n",
    "\n",
    "for i in range(len(clusters)%n_divisions):\n",
    "    cluster_sets[i].append(clusters[subset_size*n_divisions + i])\n",
    "\n",
    "print(f\"Combining subset's files\")\n",
    "for idx, cluster_set in enumerate(cluster_sets):\n",
    "    (output/\"raw\").mkdir(parents=True, exist_ok=True)\n",
    "    x_subset_file = open(output/f\"raw/x_train_subset_{idx+1}.csv\", \"wb\")\n",
    "    y_subset_file = open(output/f\"raw/y_train_subset_{idx+1}.csv\", \"wb\")\n",
    "\n",
    "    for cluster in cluster_set:\n",
    "        for folder in [x for x in cluster.iterdir() if x.is_dir()]:\n",
    "            for file in [x for x in folder.iterdir() if x.is_file()]:\n",
    "                df = pd.read_csv(file)\n",
    "\n",
    "                time_sentitive_features = [feature for feature in df.columns if feature not in labels and feature not in identifiers]\n",
    "                np.savetxt(x_subset_file, df[time_sentitive_features].values, delimiter=\",\", fmt=\"%5.2f\")\n",
    "                np.savetxt(y_subset_file, df[[\"5-day-predict\"]].values, delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "    x_subset_file.close()\n",
    "    y_subset_file.close()\n",
    "\n",
    "print(\"Undersampling\")\n",
    "for idx, cluster_set in enumerate(cluster_sets):\n",
    "    (output/\"under\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    X_train = np.loadtxt(output/f\"raw/x_train_subset_{idx+1}.csv\", delimiter=\",\")\n",
    "    y_train = np.loadtxt(output/f\"raw/y_train_subset_{idx+1}.csv\", delimiter=\",\")\n",
    "\n",
    "    final_X_array, final_y_array = rus.fit_resample(X_train, y_train)\n",
    "    final_X_array = zip(final_X_array, rus.sample_indices_)\n",
    "    final_y_array = zip(final_y_array, rus.sample_indices_)\n",
    "\n",
    "    final_X_array = sorted(final_X_array, key=lambda x : x[1])\n",
    "    final_y_array = sorted(final_y_array, key=lambda x : x[1])\n",
    "\n",
    "    final_X_array = [x[0] for x in final_X_array]\n",
    "    final_y_array = [x[0] for x in final_y_array]\n",
    "\n",
    "    np.savetxt(output/f\"under/x_train_subset_{idx+1}.csv\", final_X_array, delimiter=\",\", fmt=\"%5.2f\")\n",
    "    np.savetxt(output/f\"under/y_train_subset_{idx+1}.csv\", final_y_array, delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "    x_subset_file.close()\n",
    "    y_subset_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate for a single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed/train/DENSE_TREE/RL_X;OEL/A1BQ_time_sentitive_features.csv\n",
      "preprocessed/train/DENSE_TREE/RL_X;OEL/A1WD_time_sentitive_features.csv\n",
      "preprocessed/train/VERYHIGH-SPARSE_BLOCK_BUILDINGS/RL_SECSF/A8CN_time_sentitive_features.csv\n",
      "preprocessed/train/OPEN_IN_URBAN/RL_U7MRV/A8DY_time_sentitive_features.csv\n",
      "preprocessed/train/OPEN_IN_URBAN/RL_[KBEO/A7XT_time_sentitive_features.csv\n",
      "preprocessed/train/INDUSTRIAL_&_COMMERCIAL/RL_SKDCI/A2AB_time_sentitive_features.csv\n",
      "preprocessed/train/INDUSTRIAL_&_COMMERCIAL/RL_SKDCI/A2SI_time_sentitive_features.csv\n",
      "preprocessed/train/AVERAGE-MEDIUM_URBAN/RL_S9LQB/A2BA_time_sentitive_features.csv\n",
      "preprocessed/train/AVERAGE-MEDIUM_URBAN/RL_S9LQB/A2IS_time_sentitive_features.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = f\"./divided/train/{1}_workers/\"\n",
    "output = pathlib.Path(output_path)\n",
    "output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(output/\"raw\").mkdir(parents=True, exist_ok=True)\n",
    "x_file = open(output/f\"raw/x_train.csv\", \"wb\")\n",
    "y_file = open(output/f\"raw/y_train.csv\", \"wb\")\n",
    "\n",
    "# Merging files into the number of workers\n",
    "for cluster in clusters:\n",
    "    for folder in [x for x in cluster.iterdir() if x.is_dir()]:\n",
    "        for file in [x for x in folder.iterdir() if x.is_file()]:\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            time_sentitive_features = [feature for feature in df.columns if feature not in labels and feature not in identifiers]\n",
    "            np.savetxt(x_file, df[time_sentitive_features].values, delimiter=\",\", fmt=\"%5.2f\")\n",
    "            np.savetxt(y_file, df[[\"5-day-predict\"]].values, delimiter=\",\", fmt=\"%d\")\n",
    "            if df[time_sentitive_features].values.shape[1] != 620:\n",
    "                print(file)\n",
    "\n",
    "x_file.close() \n",
    "y_file.close()\n",
    "\n",
    "#Undersampling\n",
    "(output/\"under\").mkdir(parents=True, exist_ok=True)\n",
    "X_train = np.loadtxt(output/\"raw/x_train.csv\", delimiter=\",\")\n",
    "y_train = np.loadtxt(output/\"raw/y_train.csv\", delimiter=\",\")\n",
    "\n",
    "final_X_array, final_y_array = rus.fit_resample(X_train, y_train)\n",
    "final_X_array = zip(final_X_array, rus.sample_indices_)\n",
    "final_y_array = zip(final_y_array, rus.sample_indices_)\n",
    "\n",
    "final_X_array = sorted(final_X_array, key=lambda x : x[1])\n",
    "final_y_array = sorted(final_y_array, key=lambda x : x[1])\n",
    "\n",
    "final_X_array = [x[0] for x in final_X_array]\n",
    "final_y_array = [x[0] for x in final_y_array]\n",
    "\n",
    "np.savetxt(output/\"under/x_train.csv\", final_X_array, delimiter=\",\", fmt=\"%5.2f\")\n",
    "np.savetxt(output/\"under/y_train.csv\", final_y_array, delimiter=\",\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['AVERAGE-DENSE URBAN', 'AVERAGE-MEDIUM URBAN',\n",
      "       'AVERAGE-SPARSE URBAN', 'BUILTUP-VILLAGE', 'DENSE TREE',\n",
      "       'GREEN HOUSE', 'HIGH-DENSE URBAN', 'HIGH-ISOLATED-BUILDINGS',\n",
      "       'HIGH-MEDIUM URBAN', 'HIGH-SPARSE URBAN',\n",
      "       'INDUSTRIAL & COMMERCIAL', 'INLAND WATER', 'LOW-DENSE URBAN',\n",
      "       'LOW-MEDIUM URBAN', 'LOW-SPARSE URBAN', 'OPEN IN URBAN',\n",
      "       'OPEN LAND', 'SPARSE TREE', 'VERYHIGH-DENSE BLOCK BUILDINGS',\n",
      "       'VERYHIGH-MEDIUM BLOCK BUILDINGS',\n",
      "       'VERYHIGH-SPARSE BLOCK BUILDINGS'], dtype=object), array([289, 214,  31,   7,  74,   1,  34,   5,  21,  17, 135,   1, 148,\n",
      "       185,  46, 333,  47,  72,   2,   6,   6]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "input_path = \"train\"\n",
    "input = pathlib.Path(input_path)\n",
    "\n",
    "distances_file = input/\"distances.tsv\"\n",
    "rl_sites_file = input/\"rl-sites.tsv\"\n",
    "\n",
    "distances_df = pd.read_csv(distances_file, sep=\"\\t\", index_col=0)\n",
    "rl_sites_df = pd.read_csv(rl_sites_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "#print(np.unique(rl_sites_df[\"clutter_class\"], return_counts=True))\n",
    "\n",
    "rl_stations = rl_sites_df[\"site_id\"].unique()\n",
    "\n",
    "distances_df = distances_df.loc[rl_stations, rl_stations]\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=8, metric='precomputed', linkage='complete').fit(distances_df.values)\n",
    "\n",
    "labels = clustering.labels_\n",
    "\n",
    "print(np.unique(labels, return_counts=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow_vs_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
